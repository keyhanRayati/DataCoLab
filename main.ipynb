{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install transformers datasets evaluate rouge_score\n",
    "! pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('booksummaries.csv', delimiter='\\t' , names=[\"Path\", \"Title\", \"Author\",\"Date\", \"Info\", \"Text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our project primarily relies on the 'Text' column, which is free from NaN values. To maintain data completeness, any missing values in other columns are replaced with designated values rather than being removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Author'] = df['Author'].fillna(\"unknown\")\n",
    "df['Date'] = df['Date'].apply(lambda x: int(x[:4]) if isinstance(x, str) and x[:4].isdigit() else -1)\n",
    "df['Info'] = df['Info'].fillna(\"{}\")\n",
    "df = df.reset_index(drop = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A.D.: New Orleans After the Deluge'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[12154]['Title'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the dataset's genre distribution. Show the top 10 most frequent genres visually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = {}\n",
    "info = df.Info.to_list()\n",
    "for i in range(len(info)):\n",
    "    tmp = json.loads(info[i])\n",
    "    for _, value in tmp.items():\n",
    "        if value in genres:\n",
    "            genres[value] += 1\n",
    "        else:\n",
    "            genres[value] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_genres = dict(sorted(genres.items(), key=lambda x: x[1], reverse=True))\n",
    "top_10_genres = dict(list(sorted_genres.items())[:10])\n",
    "\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "palette = sns.color_palette(\"tab10\")\n",
    "\n",
    "sns.barplot(x=list(top_10_genres.keys()), y=list(top_10_genres.values()), palette=palette)\n",
    "\n",
    "\n",
    "plt.title('Genre Count', fontsize=18)\n",
    "plt.xlabel('Genres', fontsize=14)\n",
    "plt.ylabel('Count', fontsize=14)\n",
    "\n",
    "plt.xticks(fontsize=10)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = df[df['Date'] > 1800]['Date'].tolist()\n",
    "genre = df[df['Date'] > 1800]['Info'].tolist()\n",
    "years = []\n",
    "genras = []\n",
    "for i in range(len(genre)):\n",
    "    tmp = json.loads(genre[i])\n",
    "    for _, value in tmp.items():\n",
    "        if value in ['Science Fiction', 'Crime Fiction', 'Speculative fiction', 'Horror']:\n",
    "            years.append(year[i])\n",
    "            genras.append(value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "sns.kdeplot(x=years , hue = genras, shade=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Density')\n",
    "plt.title('Density Plot of Years')\n",
    "plt.legend(genras, loc='upper left', title=\"Genre\")\n",
    "plt.title('Density Plot of Years', fontsize=16)\n",
    "\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 512, but your input_length is only 18. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a very long list of text I do not want to read, replace me with you. Replace me.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "summarizer = pipeline(\n",
    "    \"summarization\",\n",
    "    \"pszemraj/long-t5-tglobal-base-16384-book-summary\",\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    ")\n",
    "\n",
    "long_text = \"Here is a lot of text I don't want to read. Replace me\"\n",
    "\n",
    "result = summarizer(long_text)\n",
    "print(result[0][\"summary_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty list to store the summaries\n",
    "summaries = []\n",
    "\n",
    "# Use tqdm to iterate over the rows of the DataFrame\n",
    "for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Processing rows\"):\n",
    "    # Apply summarization function to the 'Text' column of each row\n",
    "    summary = summarizer(row[\"Text\"])\n",
    "    summaries.append(summary)\n",
    "    if (index) % 5 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# result_file = open(\"Summaries.csv\",'wb')\n",
    "# with open('output2.csv','w') as result_file:\n",
    "#     wr = csv.writer(result_file, dialect='excel')\n",
    "#     wr.writerows(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Summaries = pd.read_csv('Summaries.csv' , names=['Summaries'])\n",
    "Summaries.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = Summaries['Summaries'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "\n",
    "# # Assuming you have a list named 'text_list' containing the text values\n",
    "\n",
    "# # Specify the file path where you want to save the CSV file\n",
    "# csv_file_path = 'Summaries.csv'\n",
    "\n",
    "# # Open the CSV file in 'w' mode (write mode)\n",
    "# with open(csv_file_path, 'w', newline='', encoding='utf-8') as csv_file:\n",
    "#     # Create a CSV writer object\n",
    "#     csv_writer = csv.writer(csv_file)\n",
    "    \n",
    "#     # Write each string from the text_list to the CSV file\n",
    "#     for text in a1:\n",
    "#         csv_writer.writerow([text])\n",
    "\n",
    "# print(\"CSV file saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(summaries)):\n",
    "    summaries[i] = json.loads(summaries[i]\n",
    "                              .replace('\\\\', '')\n",
    "                              .replace('\"', '\\\\\"')\n",
    "                              .replace('{\\'summary_text\\': \\'', '{\"summary_text\": \"')\n",
    "                              .replace('{\\'summary_text\\': \\\\\"', '{\"summary_text\": \"')\n",
    "                              .replace('{\\\\\"summary_text\\\\\": \\\\\"', '{\"summary_text\": \"')\n",
    "                              .replace('\\'}', '\"}')\n",
    "                              .replace('\\\\\\\"}', '\"}')\n",
    "                              )['summary_text']\n",
    "    # Summaries[i] = Summaries[i].replace('summary_text:', '')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Summaries'] = summaries\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text-to-Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95072b41b4a74f7dbcffb00861d271f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3633746770.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[62], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    set COMMANDLINE_ARGS=--precision full --no-half --medvram --opt-split-attentionset\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mort is a teenager who has a personality that makes him unsuited for the family farm. Lezek takes him to an apprenticeship fair hoping that he will find a job as an undertaker. When he finds one, he accepts and becomes Death's apprentice. When it is time to die, Mort saves the princess from death. Both she and Mort consult Igneouscutwell for assistance. As Mort becomes more like Death, reverting to his former self, tries to feel human emotion. The duel ends in a duel between Mort and Death\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sir Charles is found dead in his country house. He has a heartattack, and Mortimer goes to London to ask Holmes for assistance. The Baskerville household is plagued with a curse that causes Hugo\\'s daughter to escape from him. When she escapes, he offers his soul to \"the devil\" if the monster can rescue her. Sir Henry arrives from America and receives an anonymous note telling him to avoid Devon moorlands. A new boot is stolen by Sir Henry, but it does not explain why. They find another one missing. Sir John returns to England and tells Holmes and Watson about the disappearance of two more boots. On the way back, they meet Jack Staplton, a local naturalist who has been living on the area for over two years. Beryl warns him against leaving the area because Selden is being pursued. Barrymore also tells Watson that Selden was once married to Mrs. Baronet, but now he refuses to marry her. Meanwhile, Sir Henry keeps trying to get Beryl to agree to help him divorce her sister. Finally, Frankfland invites Watson into his office to celebrate his law cases. In the meantime, Watson finds a message from Holmes saying that he knows where Holmes is staying. It turns out that Holmes has kept this secret so that no one will be tempted away. As soon as Watson leaves, Staptons body is found. His wife is bound and tied up in Merripit House; when she is released, she informs them of what happened. She then tells them how Stapleton hides himself in the Great Gridpen Mire: an island deep within the Great Grim Pen Mire.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[84]['Summaries']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"piece of property that thorin used to hold. he refuses to give up his claim to the treasure because he doesn 't want to fight another war. but when gandalsf warns everyone about an army approaching, they manage to win the battle without any trouble.\"]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead74fa252ca4df9acfbce6601177d94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m prompt \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m85\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSummaries\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mpipe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mimages[\u001b[38;5;241m0\u001b[39m]  \n\u001b[0;32m      4\u001b[0m image\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest.png\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Keyhan\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Keyhan\\miniconda3\\Lib\\site-packages\\diffusers\\pipelines\\stable_diffusion\\pipeline_stable_diffusion.py:1022\u001b[0m, in \u001b[0;36mStableDiffusionPipeline.__call__\u001b[1;34m(self, prompt, height, width, num_inference_steps, timesteps, guidance_scale, negative_prompt, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, ip_adapter_image, ip_adapter_image_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     do_denormalize \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mTrue\u001b[39;00m] \u001b[38;5;241m*\u001b[39m image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1022\u001b[0m     do_denormalize \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhas_nsfw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhas_nsfw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhas_nsfw_concept\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m   1024\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_processor\u001b[38;5;241m.\u001b[39mpostprocess(image, output_type\u001b[38;5;241m=\u001b[39moutput_type, do_denormalize\u001b[38;5;241m=\u001b[39mdo_denormalize)\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# Offload all models\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not iterable"
     ]
    }
   ],
   "source": [
    "prompt = df.iloc[85]['Summaries']\n",
    "image = pipe(prompt).images[0]  \n",
    "\n",
    "image.save(\"test.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index[0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "photo_names = []\n",
    "\n",
    "df2 = df.iloc[13645:, :]\n",
    "\n",
    "for index, row in tqdm(df2.iterrows(), total=len(df2), desc=\"Processing rows\"):\n",
    "    name = str(index)+'_'+str(row['Title'])+'.png'\n",
    "    prompt = row['Summaries']\n",
    "    photo = pipe(prompt).images[0]\n",
    "    try:\n",
    "        photo.save(f'images/{name}') \n",
    "    except:\n",
    "        photo.save(str(index)+'.png')\n",
    "    photo_names.append(name)\n",
    "    if (index) % 5 == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## mistakes correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some files didn't saved properly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "arr = os.listdir('images/')\n",
    "\n",
    "for i, value in tqdm(enumerate(arr), total=len(arr)):\n",
    "    if value.endswith('.png'):\n",
    "        continue\n",
    "\n",
    "    else:\n",
    "        try:\n",
    "            os.remove(f\"images/{value}\")\n",
    "            name = value + '.png'\n",
    "            index = int(value.split('_')[0])\n",
    "            prompt = df.loc[index]['Summaries']\n",
    "            photo = pipe(prompt).images[0]\n",
    "            try:\n",
    "                photo.save(f'images/{name}') \n",
    "            except:\n",
    "                photo.save(\"images/\" + str(index)+'.png')\n",
    "            # photo_names.append(name)\n",
    "            if (i) % 5 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        except:\n",
    "            continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a function for finding for black images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is not completely black.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def is_image_all_black(image_path):\n",
    "    img = Image.open(image_path)\n",
    "    img = img.convert('L')\n",
    "    pixels = img.getdata()  \n",
    "    return all(pixel == 0 for pixel in pixels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove black images and try to replace them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16559/16559 [03:18<00:00, 83.48it/s]\n"
     ]
    }
   ],
   "source": [
    "arr = os.listdir('images/')\n",
    "\n",
    "for i, value in tqdm(enumerate(arr), total=len(arr)):\n",
    "    try:\n",
    "\n",
    "        if is_image_all_black(f'images/{value}'):\n",
    "                os.remove(f\"images/{value}\")\n",
    "                # name = value\n",
    "                # index = int(value.split('_')[0])\n",
    "                # prompt = df.loc[index]['Summaries']\n",
    "                # photo = pipe(prompt).images[0]\n",
    "                # try:\n",
    "                #     photo.save(f'images/{name}') \n",
    "                # except:\n",
    "                #     photo.save(\"images/\" + str(index)+'.png')\n",
    "                # if (i) % 5 == 0:\n",
    "                #     torch.cuda.empty_cache()\n",
    "\n",
    "    except:\n",
    "        continue\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### double summerise for bypassing the rules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = os.listdir('images/')\n",
    "\n",
    "index_list = []\n",
    "for i in arr:\n",
    "    try:\n",
    "        index_list.append(int(i.split('_')[0]))\n",
    "    except:\n",
    "        index_list.append(int(i.split('.')[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 512, but your input_length is only 52. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=26)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------- 10878 ---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 512, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n",
      "Your max_length is set to 512, but your input_length is only 35. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=17)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74981ff3c5b44f828ed7442790ded4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 512, but your input_length is only 84. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=42)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------- 11697 ---------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 512, but your input_length is only 28. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=14)\n",
      "Your max_length is set to 512, but your input_length is only 17. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=8)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a673c980203401f95537288279288eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    if not i in index_list:\n",
    "        print(f\"------------------------------------------------------- {i} ---------------------------------------------------\")\n",
    "        prompt = df.iloc[i]['Summaries']\n",
    "        prompt = summarizer(prompt)[0]['summary_text']\n",
    "        photo = pipe(prompt).images[0]\n",
    "        photo.save(f\"{i}.png\")\n",
    "\n",
    "        if (i) % 3 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
